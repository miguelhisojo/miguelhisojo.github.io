[{"authors":["admin"],"categories":null,"content":"My interests are based around Data science and information technologies, which includes Signal Processing, Machine Learning and Data analysis.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://miguelhisojo.github.io/author/miguel-angel-hisojo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/miguel-angel-hisojo/","section":"authors","summary":"My interests are based around Data science and information technologies, which includes Signal Processing, Machine Learning and Data analysis.","tags":null,"title":"Miguel Angel Hisojo","type":"authors"},{"authors":["Miguel Hisojo"],"categories":["Signal processing"],"content":"How your wireless speaker works? An FSK simulation Mobile phones, internet access, and listening the radio, those are normal activities for most of us. When any of those activities take place, we impose a data signal wave over a carrier signal wave, generally of higher frequency. This process is called modulation.\nDigital Modulation provides more information capacity, high data security, quicker system availability with great quality communication.\nFrequency Shift Keying (FSK) is one of the most common modulation techniques. FSK modulates a signal so it can be transmitted wirelessly. Bluetooth, uses Gaussian FSK modulation, a slightly different version of FSK. This notebook aims to provide an overview of how data transmission works over FSK.\nSystem description. Transmitter FSK modulation uses two frequencies to represent binary values of 1 and 0, so our input data d will be converted to these frequencies. Our transmitter (tx) will simply create two distinct frequencies based on a data value, by using a voltage-controlled oscillator (VCO).\n FSK Transmitter   The transmitted signal s(t) is defined as\n$$ s(t) = A \\cos(2 \\pi \\omega (t) ) $$\nWhere A is the signal amplitude and $\\omega (t)$ is the output of the voltage-controlled oscillator (VCO) defined as:\n$$ f_c + m(t) = \\begin{cases} 1 \u0026amp; f_c - f_{dev} \u0026amp; \\\\\n0 \u0026amp; f_c + f_{dev} \\end{cases} $$\nwhere $ f_{dev}$ is the frequency deviation, so, our output signal is represented as,\n$$ s(t) = A \\cos(2 \\pi (f_c + m(t)) * t ) $$\nLet\u0026rsquo;s code. # Import the libraries necessary for our simulation import numpy as np import pylab as pl from numpy.random import sample import scipy.signal as signal import scipy.signal.signaltools as sigtool pl.rcParams['figure.figsize'] = [15, 5] # Standard size for all plots to improve visualization. pl.rcParams['figure.dpi'] = 300 #pl.figure(figsize=(30, 10), dpi= 80)  #SIMULATED VALUES Fc = 1000 #carrier frequency of 1kHz Fbit = 50 #bitrate of data Fdev = 500 #frequency deviation N = 64 #Size of the array of bits that we will transmit A = 1 #tx ignal amplitude Fs = 10000 #sampling frequency A_n = 0.1 #noise peak amplitude N_pbits = 10 #number of bits to print in plots  #generate some random data for testing d = np.random.randint(0,1+1,N) print('d: \\n',d)  d: [0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0]  The voltage-controlled oscillator(VCO) converts the bit stream to a sine wave, with a frequency-dependent value on whether the bit is a 1 or 0. The bit stream is then extended to match the sampling frequency.\nt = np.arange(0,float(N)/float(Fbit),1/float(Fs), dtype=np.float) #extend the d to account for the bitrate and convert 0/1 to frequency m = np.zeros(0).astype(float) zero=int(Fc+Fdev) one=int(Fc-Fdev) fsbit=int(Fs/Fbit) for bit in d: if bit == 0: m=np.hstack((m,np.multiply(np.ones(int(Fs/Fbit)),Fc+Fdev))) else: m=np.hstack((m,np.multiply(np.ones(int(Fs/Fbit)),Fc-Fdev))) print(len(m)) #calculate the output of the VCO y=np.zeros(0) y=A * np.cos(2*np.pi*np.multiply(m,t)) pl.plot(t[0:int(Fs*N_pbits/Fbit )],y[0:int(Fs*N_pbits/Fbit)]) pl.xlabel('Time (s)') pl.ylabel('Amplitude (V)') pl.title('Amplitude of carrier vs time')   12800   Amplitude of carrier vs time   Frequency measurements of the FSK signal are usually stated in terms of “shift” and center frequency. The shift is the frequency difference between the mark and space frequencies. Shifts are usually in the range of 50 to 1000 Hertz. The nominal center frequency is halfway between the mark and space frequencies.\nFrequency Shift Property of the Fourier Transform:\n$$ \\begin{aligned} \\mathscr F \\left[ x(t) \\cos (2\\pi f_c t) \\right] = \\frac{1}{2} X (f-f_c) + \\\\\n\\frac{1}{2} X (f+f_c) \\end{aligned} $$\n#visualize data in time and frequency domain def visualize_data(y): N_FFT = float(len(y)) f = np.arange(0,Fs/2,Fs/N_FFT) w = np.hanning(len(y)) y_f = np.fft.fft(np.multiply(y,w)) y_f = 10*np.log10(np.abs(y_f[0:int(N_FFT/2)]/N_FFT)) return y_f,w ,f,N_FFT  y_f,w,f,N_FFT=visualize_data(y)  pl.plot(t[0:int(Fs*N_pbits/Fbit)],m[0:int(Fs*N_pbits/Fbit)]) pl.xlabel('Time (s)') pl.ylabel('Frequency (Hz)') pl.title('Original VCO output versus time')   Original VCO output versus time   pl.plot(f[0:int((Fc+Fdev*2)*N_FFT/Fs)],y_f[0:int((Fc+Fdev*2)*N_FFT/Fs)]) pl.xlabel('Frequency (Hz)') pl.ylabel('Amplitude (dB)') pl.title('Spectrum')   Signal spectrum   The channel The signal acquires noise when passing through the channel. The thermal noise $n(t)$ is assumed to be complex additive white Gaussian (AWGN) with zero mean and power spectral density $\\frac{N_0}{2}$.\nThe received signal can be expressed as: $$ r(t)= s(t)+n(t) $$\nso,\n$$ y(t)= A \\cos(2 \\pi (f_c + m(t)) * t ) +n(t) $$\nIf the noise becomes too strong, or if the amplitude of the signal becomes too weak, the data cannot be demodulated.\nIn our simulation, we will set the noise amplitude of 0.1 which creates a signal-to-noise ratio of $~14dB$. Notice the noise in the time domain corrupting the signal in the following plots.\n#create some noise noise = (np.random.randn(len(y))+1)*A_n snr = 10*np.log10(np.mean(np.square(y)) / np.mean(np.square(noise))) print(\u0026quot;SNR = %fdB\u0026quot; % snr) y=np.add(y,noise)  SNR = 13.961869dB  y_f,w,f,N_FFT=visualize_data(y) pl.plot(t[0:int(Fs*N_pbits/Fbit )],y[0:int(Fs*N_pbits/Fbit)]) pl.xlabel('Time (s)') pl.ylabel('Amplitude (V)') pl.title('Amplitude of carrier vs time') print(int(Fs*N_pbits/Fbit )) print(int(Fs*N_pbits/Fbit))  2000 2000    pl.plot(f[0:int((Fc+Fdev*2)*N_FFT/Fs)],y_f[0:int((Fc+Fdev*2)*N_FFT/Fs)]) pl.xlabel('Frequency (Hz)') pl.ylabel('Amplitude (dB)') ##pl.title('Spectrum of the received signal')   Amplitude (dB)   The receiver  A receiver architecture   Our receiver will take concepts from analog FM, which can be demodulated by converting the frequency changes to amplitude changes. Typically, FSK demodulation utilizes an analog differentiator in order to separate the data signal from the carrier frequency, followed by an envelope detector. This procedure is simple and low-power consuming.\n$$ \\begin{aligned} \\frac{dy}{dt} = -A 2\\pi \\left ( f + m(t) + t * \\frac{d m(t)}{dt} \\right) \\\\\n\\sin (2 \\pi (fc + m(t)) * t )+ \\frac{d n(t)}{dt} \\end{aligned} $$\nCombining the amplitudes in one term and taking into account that the term $\\frac{d m(t)}{dt} = 0$ due to the constant phase change we have:\n$$ \\begin{split} \\scriptstyle \\frac{dy}{dt} = A\\ 2 \\pi \\left ( \\ f + m(t) \\ \\right ) \\cdot \\ \\sin (2 \\pi (\\ fc + m(t) \\ )\\ * \\ t ) + \\frac{d n(t)}{dt} \\end{split} $$\nIn our simulation, we can ignore the phase shift, since it is constant. Nevertheless, in real life, a clock recovery or bit synchronization scheme must be applied. The differentiator is simple and is just a discrete differentiation function, it can be implemented as:\ny_diff = np.diff(y,1) print(y)  [ 1.15418648 0.56043178 -0.2061984 ... -0.76277215 -0.0358485 0.64900518]  The envelope detector separates the high-frequency carrier from the low-frequency digital data modulated onto the amplitude, this is done by using a Hilbert transform. After the envelope detection, the signal is low-pass filtered using a 100 tap FIR filter with a cutoff frequency of $2 * bitrate$.\n###################################################### # Envelope detector and low-pass filter ###################################################### y_env = np.abs(sigtool.hilbert(y_diff)) h=signal.firwin( numtaps=100, cutoff=Fbit*2, nyq=Fs/2) y_low_filt=signal.lfilter( h, 1.0, y_env) #data after adding noise N_FFT = float(len(y_low_filt)) f = np.arange(0,Fs/2,Fs/N_FFT) w = np.hanning(len(y_low_filt)) y_f = np.fft.fft(np.multiply(y_low_filt,w)) y_f = 10*np.log10(np.abs(y_f[0:int(N_FFT/2)]/N_FFT)) pl.plot(t[0:int(Fs*N_pbits/Fbit)],m[0:int(Fs*N_pbits/Fbit)]) pl.xlabel('Time (s)') pl.ylabel('Frequency (Hz)') pl.title('Original VCO output vs. time')   Original VCO output vs. time   pl.plot(t[0:int(Fs*N_pbits/Fbit)],np.abs(y[0:int(Fs*N_pbits/Fbit)])) pl.plot(t[0:int(Fs*N_pbits/Fbit)],y_low_filt[0:int(Fs*N_pbits/Fbit)],'r',linewidth=3.0) pl.xlabel('Time (s)') pl.ylabel('Amplitude (V)') pl.title('Filter signal and unfiltered signal vs. time')   Filter signal and unfiltered signal vs. time   pl.plot(f[0:int((Fc+Fdev*2)*N_FFT/Fs)],y_f[0:int((Fc+Fdev*2)*N_FFT/Fs)]) pl.xlabel('Frequency (Hz)') pl.ylabel('Amplitude (dB)') pl.title('Spectrum')   Sectrum   The slicer takes the mean of the entire filtered signal, and uses this as a decision threshold, to decide if a bit is a 1 or a 0. The decision is done at the center of the bit period. The result is saved to an array and compared to the original, to find bit errors. The bit error percentage is printed to the console.\n#calculate the mean of the signal mean = np.mean(y_low_filt) #if the mean of the bit period is higher than the mean, the data is a 0 sampled_signal = y_low_filt[int(Fs/Fbit/2):len(y_low_filt):int(Fs/Fbit)] print(sampled_signal) pl.plot(sampled_signal)  [0.94154175 0.92515726 0.35223853 0.33979885 0.35294071 0.36287615 0.34675628 0.90095976 0.35453665 0.92090498 0.34720447 0.91611558 0.94528336 0.34417705 0.90435061 0.91620578 0.34844842 0.3388747 0.94404786 0.34364565 0.35304868 0.356202 0.35102639 0.34859071 0.33627503 0.3383037 0.93701699 0.92527765 0.91127889 0.92546922 0.90017475 0.34158569 0.34268534 0.92516404 0.91248606 0.33776427 0.3454749 0.93246055 0.34503893 0.34917072 0.35666379 0.90326687 0.35272848 0.92576273 0.35310379 0.34613621 0.38067857 0.88376517 0.35971779 0.88856201 0.93363287 0.89404895 0.34008713 0.34691357 0.33626758 0.90701975 0.33288698 0.91293492 0.34244479 0.36333365 0.33606437 0.95159041 0.922426 0.93333146]   Received sequence.   rx_data=[] [rx_data.append(0) if bit \u0026gt; mean else rx_data.append(1) for bit in sampled_signal] bit_error=0 for i in range(0,len(d)): if rx_data[i] != d[i]: bit_error+=1 print (\u0026quot;bit errors = %d\u0026quot; % bit_error) print (\u0026quot;bit error percent = %4.2f%%\u0026quot; % (float(bit_error)/float(N)*100) )  bit errors = 0 bit error percent = 0.00%  Conclusion FSK is a digital modulation technique to increase the frequency characteristics of the input binary signal. By FSK modulation technique we can achieve error-free communication in a few digital applications. Nevertheless, FSK has finite data rate and consumes more bandwidth can be overcome by the QAM, which is known as quadrature amplitude modulation. It is the combination of amplitude modulation and phase modulation.\n","date":1597242418,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597242418,"objectID":"2deefbca60a9d48be9624c4fdb64d5dc","permalink":"https://miguelhisojo.github.io/project/inside-a-wireless-transmission/","publishdate":"2020-08-12T16:26:58+02:00","relpermalink":"/project/inside-a-wireless-transmission/","section":"project","summary":"Wireless speakers, internet signals, headset, etc., are part of our daily life. Nevertheless, understanding how it works is essential to create new gadgets. This post will focus on FSK modulation. A digital modulation technique, present in our daily lives.","tags":["Signal processing","Data Science"],"title":"Inside a Wireless Transmission","type":"project"},{"authors":["Miguel Hisojo"],"categories":["Data Science"],"content":"How to see the big picture? Web scraping, the answer to get ahead in a world producing billions of data points in seconds.\nWhat is it? According to Wikipedia\u0026hellip;\n Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites. Web scraping software may access the World Wide Web directly using the Hypertext Transfer Protocol or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis.  In short: Web scraping, allows you to parse HTML code from websites and save it in a spreadsheet or database giving you data insights.\nWhy is it needed \u0026hellip; Everybody has to take decisions, and making informed decisions is key for any purpose you have. Web scraping allows you to make decisions and understand how other actors in your domain are getting ahead.\n  For businesses, this allows them to get the data, information, statistics, or knowledge of the latest trends and understand their competitors.\n  For analysts to allow them to get pricing intelligence, competitor analysis, market research, or sentiment analysis, you need to scrape actual data from the web to arrive at a suitable strategy.\n  Who needs it You need this, If you work as:\n entrepreneur recruiter marketer researcher analyst journalist data scientist medical professional politician lawyer accountant  Let\u0026rsquo;s code. For web scraping, we will need to install some python libraries like Beautiful Soup, requests and lxml\nWe need to pull data from websites, for this, we will use a Python library called Beaitiful Soup\nInstall the libraries Beautiful Soup To install Beaitiful Soup, only type in your terminal\n pip install beautifulsoup4  This will allow you to get the latest version of the library. Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.\nRequests The Requests library is the standard to make an HTTP request in python. Install the library typing in your terminal:\n pip install requests  lxml We will needa “parser” for interpreting the HTML page. The lxml library combines the speed and XML feature completeness of libxml2 and libxslt libraries with the simplicity of a native Python API, mostly compatible but superior to the well-known ElementTree API. Install the library typing in your terminal:\n pip install lxml   Example   Extracting article information from a blog.  This example will show you how web scraping is valuable for journalists and business owners. Allowing them to get data from different sources for market analysis.\nImporting our libraries We need to import our Beautiful Soup (bs4) and requests libraries as follows:\nfrom bs4 import BeautifulSoup import requests  Now we are going to import an article from the entrepreneur blog on the topic millennials For this, we are going to use the method GET from our requests library. The GET method indicates that you’re trying to get or retrieve data from a specified resource.\nThe HTTP request will send us a response 200, this code means the request was valid.\nresponse = requests.get('https://www.entrepreneur.com/topic/millennials') print(response)  \u0026lt;Response [200]\u0026gt;  We want to look at the code Now, we want to make sure that we actually extract the HTML code from the request we made. Besides, we want to see the actual text result of the HTML page, you can read the .text property of this object.\nsource=response.text #print(source)  Now, we can make the Soup We will make a variable called soup that will call the Beautiful soup constructor, this will receive a string. In our case, these will be our source variable, and our lxml parser.\nNote, that there are different parsers that we can use for this, but the difference in performance is not that big, feel free to try them all.\nsoup = BeautifulSoup(source, 'lxml')  Extracting information about the article Our soup variable contains all the HTML code of the blog, however, we want to extract only what is interesting to us, like the title of the article and the summary.\nTo find out what we need to extract, we can go to our web page and open the developer tools as follows:\n right-click on the title of an article on the menu, click on inspect  Once we have localized the HTML tags that contain the information necessary for us, we can extract it with the find function. The information we need is:\n The title Summary of the article. Link Author  The title is contained inside an a tag, as well as the link URL, the summary is inside div with class deck and the author inside a span element.\nNow, it is important to understand where these elements are wrapped on, so we can see that they are wrapped inside a div with class block\nFirst, we will begin by extracting the wrapping element. For doing this, we will use the find() attribute from bs4, that receives the element div and the class.\nThe word class is a reserved word in Python, so this function uses **class_** to differentiate between these two.\nNow we can extract a block of code and display it in our terminal, for this we will use the prettify() function. A pretty-printed block will allow you to see the indentation of the HTML code, so you can actually notice the tags and elements you need to access.\narticle = soup.find('div',class_='block') print(article.prettify())  \u0026lt;div class=\u0026quot;block\u0026quot;\u0026gt; \u0026lt;a class=\u0026quot;kicker ga-click\u0026quot; data-ga-action=\u0026quot;kicker\u0026quot; data-ga-category=\u0026quot;topic-millennials-playlist-latest-feature\u0026quot; data-ga-label=\u0026quot;playlist.1\u0026quot; href=\u0026quot;/topic/millennials\u0026quot;\u0026gt; Millennials \u0026lt;/a\u0026gt; \u0026lt;h3\u0026gt; \u0026lt;a class=\u0026quot;ga-click\u0026quot; data-ga-action=\u0026quot;headline\u0026quot; data-ga-category=\u0026quot;topic-millennials-playlist-latest-feature\u0026quot; data-ga-label=\u0026quot;playlist.1\u0026quot; href=\u0026quot;/article/348092\u0026quot;\u0026gt; 7 Interesting Financial Facts About Millennials \u0026lt;/a\u0026gt; \u0026lt;/h3\u0026gt; \u0026lt;div class=\u0026quot;deck\u0026quot;\u0026gt; The millennial generation has its virtues and shortcomings, but more often than not, millennials are considered to be financially indisciplined. \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;byline\u0026quot;\u0026gt; \u0026lt;a class=\u0026quot;ga-click\u0026quot; data-ga-action=\u0026quot;authorname\u0026quot; data-ga-category=\u0026quot;topic-millennials-playlist-latest-feature\u0026quot; data-ga-label=\u0026quot;playlist.1\u0026quot; href=\u0026quot;/author/portia-antonia-alexis\u0026quot;\u0026gt; \u0026lt;span class=\u0026quot;name\u0026quot;\u0026gt; Portia Antonia Alexis \u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;span class=\u0026quot;spacer\u0026quot;\u0026gt; | \u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;readtime\u0026quot;\u0026gt; 11 min read \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;  Extracting the headline In our previous line, we notice the title is inside an $\u0026lt;$a$\u0026gt;$ tag, which is inside an $\u0026lt;$h3$\u0026gt;$ element\n\u0026lt;h3\u0026gt; \u0026lt;a href=\u0026quot;/article/348092\u0026quot;\u0026gt;7 Interesting Financial Facts About Millennials \u0026lt;/a\u0026gt; \u0026lt;/h3\u0026gt;  We can access the HTML elements, making reference to the object containing the block code, in this case, we called article\nfor article in soup.find_all('div',class_='block'):headline= article.h3.a.text print(headline)  --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) \u0026lt;ipython-input-13-85011acff508\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 for article in soup.find_all('div',class_='block'):headline= article.h3.a.text 2 print(headline) AttributeError: 'NoneType' object has no attribute 'a'  Extracting the summary The same way, we can extrac the summary, contained inside a $\u0026lt;$div$\u0026gt;$ element with class name deck\nsumary = article.find('div',class_='deck' ).text print(sumary)  Extracting the URL The URL is an attribute href inside the title tag, we can access attributes by specifying it on brackets next to the element where it is contained as follows:\nlink = article.h3.a['href'] link = f'https://www.entrepreneur.com{link}' print(link)   Extract all the articles in a page, save it on a CSV  The find_all* method We want to get all articles, we’ll need to use the find_all() method to extract all the tags with all the blocks containing an article.\nWe need exceptions Sometimes, a block might have a missing element, url, or a description. So we will use exceptions to pass when something goes wrong.\nWe will save every article inside a CSV file. Here is the full code of this example.\nfrom bs4 import BeautifulSoup import requests import csv source = requests.get('https://www.entrepreneur.com/topic/millennials').text domain= 'https://www.entrepreneur.com/' soup = BeautifulSoup(source, 'lxml') with open('cms_file.csv','w') as csv_file: csv_writer=csv.writer(csv_file) csv_writer.writerow(['headline','summary','link']) for article in soup.find_all('div',class_='block'): try: headline= article.h3.a.text #print(headline) sumary = article.find('div',class_='deck' ).text #print(sumary) link = article.h3.a['href'] link = f'https://www.entrepreneur.com{link}' #print(link) except Exception as e: pass csv_writer.writerow([headline,sumary,link ])  The resulting CSV file Now, try it on financial websites or investment websites, and use pandas to get the most of your data! ","date":1597173753,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597173753,"objectID":"fafc510ef31393366138a542dc3938f8","permalink":"https://miguelhisojo.github.io/project/web-scraping/","publishdate":"2020-08-11T21:22:33+02:00","relpermalink":"/project/web-scraping/","section":"project","summary":"Web scraping or data mining, one of the most important tools in data science. This allows you to get millions of data points everyday, to get insighs on markets, finance, business among many others.","tags":["Data Science","Deep Learning"],"title":"WEB SCRAPING","type":"project"}]